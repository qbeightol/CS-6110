\documentclass[10pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{geometry}
\usepackage{mathpartir}
% \usepackage{parskip}
\usepackage{stmaryrd}
\usepackage{enumerate, verbatim}

\pagestyle{fancy}

\lhead{\sf CS 6110 Homework 2}
\rhead{\sf Giang Nguyen (htn26) and Quinn Beightol (qeb2)}


% Commands %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\problem}[1]{\section{\sf #1}}
\newcommand{\translate}[2]{\ensuremath {\llbracket #1 \rrbracket}_{#2}}
\newcommand{\defined}{\triangleq}
\renewcommand{\S}{\textsf{S}}
\newcommand{\K}{\textsf{K}}
\newcommand{\I}{\textsf{I}}
\newcommand{\stepsA}{\rightarrow_a}
\newcommand{\stepsB}{\rightarrow_b}
\newcommand{\steps}{\rightarrow}
\newcommand{\for}[2]{\textsf{for}~#1~\textsf{do}~#2}
\newcommand{\config}[2]{\langle#1,~#2\rangle}
\newcommand{\impSkip}{\textsf{skip}}
\newcommand{\impIf}[3]{\textsf{if}~#1~\textsf{then}~#2~\textsf{else}~#3}
\newcommand{\while}[2]{\textsf{while}~#1~\textsf{do}~#2}
\newcommand{\true}{\textsf{true}}
\newcommand{\false}{\textsf{false}}
\newcommand{\bigStepsA}{\Downarrow_a}
\newcommand{\bigStepsB}{\Downarrow_b}
\newcommand{\bigSteps}{\Downarrow}
\renewcommand{\gets}[2]{#1~\texttt{:=}~#2}
\newcommand{\Rule}[3]{
  [\textsc{#1}]~
  \label{rule:#1}
  \hfill
  \ensuremath{\inferrule{#2}{#3}}
  \hfill
}
\newcommand{\RuleWithCondition}[4]{
  [\textsc{#1}]~
  \label{rule:#1}
  \hfill
  \ensuremath{\inferrule{#2}{#3}}
  (#4)
  \hfill
}

\newtheorem{claim}{Claim}[section]
\newtheorem{lemma}{Lemma}[claim]


\begin{document}
\problem{Observational Equivalence}

\textbf{Lemma 1:}
\begin{enumerate}[(i)]
\item \textbf{Claim: } $\equiv_{obs}$ is an equivalence relation assuming that $\equiv$ is also an equivalence relation.

\begin{enumerate}
\item Consider a term $e$. We will prove that $e \equiv_{obs} e$.

For any context $C$, $C[e] \Downarrow$ if and only if $C[e]$ itself converges. In other words,  $C[e] \Downarrow$ iff  $C[e] \Downarrow$.

Suppose $C[e] \Downarrow v_1$ and $C[e] \Downarrow v_2$ under CBV evaluation. Since CBV is deterministic $v_1$ and $v_2$ must be syntactically equivalent. $\equiv$ is reflexive since it is an equivalence relation, $v_1 \equiv v_2$. Therefore $\equiv_{obs}$ is also reflexive.
\item Suppose $e_1 \equiv_{obs} e_2$ we will prove that $e_2 \equiv_{obs} e_1$.

$e_1 \equiv_{obs} e_2$ so $C[e_1] \Downarrow$ iff $C[e_2] \Downarrow \Rightarrow C[e_2] \Downarrow$ iff $C[e_1] \Downarrow$.

Suppose $C[e_1] \Downarrow v_1$ and $C[e_2] \Downarrow v_2$ under CBV evaluation then $v_1 \equiv v_2$ since $e_1 \equiv_{obs} e_2 \Rightarrow v_2 \equiv v_2$ because $\equiv$ is symmetric.

$\Rightarrow e_2 \equiv_{obs} e_1$. $\equiv_{obs}$ is symmetric.
\item Let $e_1, e_2$ and $e_3$ be $3$ terms such that $e_1 \equiv_{obs} e_2$ and $e_2 \equiv_{obs} e_3$.

For any context $C$, $C[e_1] \Downarrow$ if and only if $C[e_2] \Downarrow$ if and only if $C[e_3] \Downarrow$.

Suppose $C[e_1] \Downarrow v_1, C[e_2] \Downarrow v_2$ and $C[e_3] \Downarrow v_3$ then $v_1 \equiv v_2$ and $v_2 \equiv v_3 \Rightarrow v_1 \equiv v_3$ as $\equiv$ is transitive.

Thus $\equiv_{obs}$ is transitive. $\Box$
\end{enumerate}
\item \textbf{Claim}: $\equiv_{obs}$ refines $\equiv$.

Let $v_1$ and $v_2$ be $2$ values such that $v_1 \equiv_{obs} v_2$. Let $C[.]$ be the context $[.]$ that only has a hole.

$C[v_1] = v_1$ and $C[v_2] = v_2$.

$C[v_1] \equiv C[v_2]$ since   $v_1 \equiv_{obs} v_2 \Rightarrow v_1 \equiv v_2 \Box$.
\item This part is exactly the first condition in the definition of $\equiv_{obs}$.
\item Suppose $\equiv^1$ refines $\equiv^2$ and $e_1 \equiv^1_{obs} e_2$. We will prove that $e_1 \equiv^2_{obs} e_2$.

$e_1 \equiv^1_{obs} e_2$ so for any context $C$, $C[e_1] \Downarrow$ if and only if $C[e_2] \Downarrow$.

If $C$ is a context such that $C[e_1] \Downarrow v_1$ and $C[e_2] \Downarrow v_2$ then $v_1 \equiv^1 v_2 \Rightarrow v_1 \equiv^2 v_2 \Box$.
\end{enumerate}

%TODO
%Fix the indentation
\textbf{Theorem 2}: For CBV and CBN, $\equiv_{\Downarrow}$ is a fixed point of the transformation $\equiv$  $\mapsto$  $\equiv_{obs}$  and is the coarsest such fixed point.

First we will prove that $e_1 \equiv_{\Downarrow} e_2$ if and only if $e_1  (\equiv_{\Downarrow})_{obs} e_2$.

$(\Rightarrow)$ Suppose  $e_1 \equiv_{\Downarrow} e_2$ then by definition for any context $C$, $C[e_1] \Downarrow$ iff $C[e_2] \Downarrow$.

Consider an arbitrary context $C$ such that $C[e_1] \Downarrow v_1$ and $C[e_2] \Downarrow v_2$ under CBV evaluation rules. In fact this entire proof can be used to prove the case for CBN evaluation just by replacing CBV with CBN.

%TODO
%Complete this direction of the proof

$(\Leftarrow)$ This follows directly from (ii) of Lemma 1. $(\equiv_{\Downarrow})_{obs}$ refines $\equiv_{\Downarrow}$.\\

Next we need to show that if $\equiv$ is a fixed point then it refines $\equiv_{\Downarrow}$.

Suppose $e_1 \equiv e_2$ then since $\equiv = \equiv_{obs}$, $e_1 \equiv_{obs} e_2$. 

So for any context $C$, $C[e_1] \Downarrow$ iff $C[e_2] \Downarrow \Rightarrow e_1 \equiv_{\Downarrow} e_2~ \Box$. 

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\problem{Set Operators and Fixed Points}

\textbf{Claim}: Given any set $A$ there is a least fixed point of $\overline R$.\\

\textbf{Proof:} Suppose $\overline R$ is a set operator on some set $N$ then $\overline R : N \rightarrow N$ and $\overline R (N) \subseteq N$.

But $N \subseteq \overline R (N)$ by definition of $\overline{N} \Rightarrow N = \overline{R} (N)$. $N$ is a fixed point.

Let $\overline{A} = \bigcap \{A' | \overline{R} (A') = A' \text{ and } A \subseteq A'\}$. In other words $\overline{A}$ is the intersection of all the fixed point containing $A$. We know that there is at least one such fixed point, i.e. $N$ itself.

First it's easy to see that $\overline{A}$ contains $A$.

For any arbitrary fixed point $A'$ that contains $A$, $\overline{A} \subseteq A'\Rightarrow \overline{R} (\overline{A}) \subseteq \overline R (A') = A'$ by the monotinicity of $\overline R$. Since this is true for any fixed point $A'$ containing $A$,  $\overline{R} (\overline{A}) \subseteq \bigcap \{A' | \overline{R} (A') = A' \text{ and } A \subseteq A'\} = \overline{A}$.

$\overline{A} \subseteq \overline{R} (\overline{A})$ so $\overline{A} = \overline{R} (\overline{A})$ and $\overline{A}$ is also a fixed point containing $A$.

$\overline{A}$ is the least among all such fixed points as if $A'$ is also a fixed point containing $A$, $\overline{A} \subseteq A' \Box$.\\

To get an example of set operator that is monotonic but not increasing, consider a set operator $S$ on the natural numbers $\mathbb{N}$: $S (A)\defined \{n+1 | n \in A\}$.

If $A \subseteq B$ and $x \in S(A) \Rightarrow x-1 \in A \Rightarrow x-1 \in B \Rightarrow x \in S(B) \Rightarrow S(A) \subseteq S(B)$ so $S$ is monotonic.

$S\{0\} = \{1\} \nsupseteq \{0\}$ so obviously $S$ is not increasing.

Let $A = \{0\}$. To see that there is no least fixed point of $S$ that contains $A$ consider any nonempty set $B \subset \mathbb{N}$. There must exist a least element in $B$ (using the normal $\leq$ ordering). Let $x$ be this minimum element then it is easy to see that $x \notin S(B)$ else $x-1 \in B$ which contradicts to the assumption that no element less than $x$ belongs to $B$. So $B$ cannot be a fixed point of $S$. In fact the only fixed point of $S$ is $\varnothing$.

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\problem{Combinators} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate}[(a)]
  \item $\I \defined \S~\K~\K$

  $\S~\K~\K~x \rightarrow \K~x~(\K~x) \rightarrow x$
  \item $ \lambda x y.y \defined \S~\K$

  $\S~\K~x~y \rightarrow \K~x(y~x) \rightarrow x$
  \item $\Omega \defined (\S~ \I~ \I) (\S~ \I~ \I)$

  $\S~ \I~ \I~x \rightarrow \I ~ x ~ (\I ~ x) \rightarrow x~x$
  \item
  \item

  % NOTE: this translation doesn't work. Try translating something with a
  % top-level application (ex: Î©). But it might have some salvageable ideas.

  Use $\translate{e}{V}$ to translate a lambda term $e$ in a context
  where the list of variables $V$ are in scope. The order of variables indicate
  when the variable was introduced. E.g. the variables for translating $e$ in
  $\lambda x. \lambda y. e$ would be $V = x, y$)

  \begin{eqnarray*}
    \translate{x}{x, V}         & \defined & \I                  \\
    \translate{x}{y, V}         & \defined & \K~\translate{x}{V} \\
    \translate{\lambda x. e}{V} & \defined & \translate{e}{V,x}  \\
    \translate{e_1~e_2}{V}      & \defined & \S~\translate{e_1}{V}~\translate{e_2}{V}
  \end{eqnarray*}

  To translate closed $\lambda$-terms, use the empty context (denoted $\cdot$).

\end{enumerate}
\problem{For Loops} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate}[(a)]
  \item
  \Rule{LoopEvalA}{
    \config{a}{\sigma} \stepsA a'
  }{
    \config{\for{a}{c}}{\sigma} \steps \config{\for{a'}{c}}{\sigma}
  } \\*

  \RuleWithCondition{LoopExit}{}{
    \config{\for n c}{\sigma} \steps \config{\impSkip}{\sigma}
  }{$n \leq 0$} \\*

  \RuleWithCondition{LoopEvalBody}{}{
    \config{\for n c}{\sigma} \steps \config{c; \for{n - 1}{c}}{\sigma}
  }{$n > 0$} \\*

  \item
  \RuleWithCondition{LoopSkip}{
    a \bigStepsA n
  }{
    \config{\for a c}{\sigma} \bigSteps \sigma
  }{$n \leq 0$} \\*

  \RuleWithCondition{LoopUnroll}{
    a \bigStepsA n \\
    \forall i \in [1, n].~\config{c}{\sigma_i} \bigSteps \sigma_{i+1}
  }{
    \config{\for a c}{\sigma_1} \bigSteps \sigma_{n+1}
  }{$n > 0$} \\*

  \item
  % \underline{Claim:} \\*
  \begin{claim} \label{claimFour}
    If   $\config{c}{\sigma} \bigSteps \sigma_1$
    and  $\config{c}{\sigma} \bigSteps \sigma_2$
    then $\sigma_1 = \sigma_2$
  \end{claim}

  \begin{proof} By induction on the derivation of $\config{c}{\sigma} \bigSteps
    \sigma_1$ and a case analysis on the last rule used in that derivation.
    We'll also use the following lemmas:

    \begin{lemma} \label{evalA}
      If   $\config{a}{\sigma} \bigStepsA n_1$
      and  $\config{a}{\sigma} \bigStepsA n_2$
      then $n_1 = n_2$.
    \end{lemma}

    \begin{lemma} \label{evalB}
      If   $\config{b}{\sigma} \bigStepsB t_1$
      and  $\config{b}{\sigma} \bigStepsB t_2$
      then $t_1 = t_2$.
    \end{lemma}

    Additionally, whenever the derivation for $\config{c}{\sigma} \bigSteps
    \sigma_1$ uses a rule $R$ at it's base, the
    derivation for $\config{c}{\sigma} \bigSteps \sigma_2$ must also have $R$
    at its base. This stems from the fact that only a few rules apply to each
    syntactic form of $c$ (and frequently, just 1 rule applies). And among the
    multiple rules that apply to some forms (e.g. {\tt while}), it's impossible
    to use one rule for one $\bigSteps$ judment and not the other $\bigSteps$
    judgment---that would
    contradict either \ref{evalA} or \ref{evalB}. \\*

    \underline{\sc Skip:} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    $c = \impSkip$ so the the only applicable evaluation rule
    is $\config{c}{\sigma} \bigSteps \sigma$. Both $\sigma_1$ and $\sigma_2$
    must equal $\sigma$. \\*

    \underline{\sc Assignment:} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Here, $c = x := a$ and the derivation for $\config{c}{\sigma} \bigSteps
    \sigma_1$ takes the following form:

    $$\inferrule{
      \config{a}{\sigma} \bigStepsA n_1
    }{
      \config{x := a}{\sigma} \bigSteps \sigma[n_1 / x]
    }$$

    Because $c$ is an assignment, the last step in the $\config{c}{\sigma}
    \bigSteps \sigma_1$ derivation must use IMP's evaluation assignment rule:

    $$\inferrule{
      \config{a}{\sigma} \bigStepsA n_2
    }{
      \config{x := a}{\sigma} \bigSteps \sigma[n_2 / x]
    }$$

    Applying \ref{evalA} provides $n_1 = n_2$, which implies $\sigma_1 =
    \sigma_2$. \\*

    \underline{Sequence:} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    $$ c = c_1; c_2 $$

    $$ \inferrule{
      \config{c_1}{\sigma}    \bigSteps \sigma_1'\\
      \config{c_2}{\sigma_1'} \bigSteps \sigma_1 \\
    }{
      \config{c_1; c_2}{\sigma} \bigSteps \sigma_1 \\
    }
    \hspace{1cm}
    \inferrule{
      \config{c_1}{\sigma}    \bigSteps \sigma_2'\\
      \config{c_2}{\sigma_2'} \bigSteps \sigma_2 \\
    }{
      \config{c_1; c_2}{\sigma} \bigSteps \sigma_2 \\
    }
    $$ \\*

    Applying the inductive hypothesis yields $\sigma_1' = \sigma_2'$, and
    applying the IH again provides $\sigma_1 = \sigma_2$.

    \underline{If-True:} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    $$ c = \impIf{b}{c_t}{c_f} $$

    $$ \inferrule{
      \config{b}{\sigma} \bigStepsB \true \\
      \config{c_t}{\sigma} \bigSteps \sigma_1 \\
    }{
      \config{\impIf{b}{c_t}{c_f}}{\sigma} \bigSteps \sigma_1 \\
    }$$

    Additionally, the derivation of $\config{\impIf{b}{c_t}{c_f}}{\sigma}
    \bigSteps \sigma_2$ must use the \textsc{IfTrue} rule:

    $$ \inferrule{
      \config{b}{\sigma} \bigStepsB \true \\
      \config{c_t}{\sigma} \bigSteps \sigma_2 \\
    }{
      \config{\impIf{b}{c_t}{c_f}}{\sigma} \bigSteps \sigma_1 \\
    } $$

    If the derivation used the \textsc{IfFalse} rule, that would imply
    $\config{b}{\sigma} \bigStepsB \false$, which contradicts \ref{evalB}. \\*

    Applying the IH gives $\sigma_1 = \sigma_2$. \\*

    \underline{If-False:} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    $$ c = \impIf{b}{c_t}{c_f} $$

    $$\inferrule{
      \config{b}{\sigma} \bigStepsB \false \\
      \config{c_f}{\sigma} \bigSteps \sigma_1 \\
    }{
      \config{\impIf{b}{c_t}{c_f}}{\sigma} \bigSteps \sigma_1 \\
    }
    \hspace{1cm}
    \inferrule{
      \config{b}{\sigma} \bigStepsB \false \\
      \config{c_f}{\sigma} \bigSteps \sigma_2 \\
    }{
      \config{\impIf{b}{c_t}{c_f}}{\sigma} \bigSteps \sigma_2 \\
    }
    $$

    Again, the latter derivation must use \textsc{IfFalse}---using the other
    rule would contradict \ref{evalA}. Applying the IH gives
    $\sigma_1 = \sigma_2$. \\*

    \underline{\textsc{WhileTrue:}} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % Here, $$ c = \while b c' $$ and either \textsc{WhileTrue} was used to derive
    % both $\bigSteps$ judgments or \textsc{WhileFalse} is the last judgment of
    % both derivations. Again only the while rules are applicable to $c$, so if
    % the previous claim weren't true, it would contradict \ref{evalB}.

    Here, $c = \while b {c'}$ and both $\bigSteps$ jugdments must use the
    \textsc{WhileTrue} rule at their base:

    $$\inferrule{
      \config{b}{\sigma} \bigStepsB \true \\
      \config{c'}{\sigma} \bigSteps \sigma_1
    }{
      \config{\while b {c'}}{\sigma} \bigSteps \sigma_1 \\
    }
    \hspace{1cm}
    \inferrule{
      \config{b}{\sigma} \bigStepsB \true \\
      \config{c'}{\sigma} \bigSteps \sigma_2
    }{
      \config{\while b {c'}}{\sigma} \bigSteps \sigma_2 \\
    }
    $$

    Applying the IH provides $\sigma_1 = \sigma_2$ \\*

    \underline{\sc WhileFalse:} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Again, $c = \while b {c'}$ and both $\bigSteps$ judgment derivations must
    end with the \textsc{WhileFalse} rule:

    $$
    \inferrule{
      \config{b}{\sigma} \bigStepsB \false
    }{
      \config{c'}{\sigma} \bigSteps \sigma
    }
    $$

    This fact implies that $\sigma_1$ and $\sigma_2$ are both equal to
    $\sigma$. \\*

    \underline{\sc LoopSkip:} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    $$ c = \for{a}{c'} $$

    $$\inferrule{
      a \bigStepsA n
    }{
      \config{\for{a}{c'}}{\sigma} \bigSteps \sigma
    }(n \leq 0)
    \hspace{1cm}
    \inferrule{
      a \bigStepsA n
    }{
      \config{\for{a}{c'}}{\sigma} \bigSteps \sigma
    }(n \leq 0)
    $$

    Both $\sigma_1$ and $\sigma_2$ are $\sigma$, so $\sigma_1 = \sigma_2$. \\*

    \underline{\sc LoopUnroll:} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    $$ c = \for{a}{c'} $$

    $$ \inferrule{
      a \bigStepsA n \\
      \forall i \in [1, n].~\config{c'}{\sigma^1_i} \bigSteps \sigma^1_{i+1}
    }{
      \config{\for a {c'}}{\sigma^1_1} \bigSteps \sigma^1_{n+1}
    }(n > 0)
    \hspace{1cm}
    \inferrule{
      a \bigStepsA n \\
      \forall i \in [1, n].~\config{c'}{\sigma^2_i} \bigSteps \sigma^2_{i+1}
    }{
      \config{\for a {c'}}{\sigma_1} \bigSteps \sigma^2_{n+1}
    }(n > 0)
    $$

    Where $\sigma_1 = \sigma^1_1 = \sigma^2_2$, $\sigma_1 = \sigma^1_{n+1}$ and
    $\sigma_2 = \sigma^2_{n+1}$ \\*

    We can repeatedly apply the inductive hypothesis to see that $\sigma^1_i
    = \sigma^2_1$ for any integer $i$ in $[1, n+1]$; $\sigma^1_0 =
    \sigma^2_0$ comes from our assumptions, and if $\sigma^1_i = \sigma^2_i$
    for some $i$, then we can apply the IH (for \ref{claimFour}) to see that
    $\sigma^1_{i+1} = \sigma^2_{i+1}$. Consequently, $\sigma_1 = \sigma^1_{n+1}
    = \sigma^2_{n+1} = \sigma^2$.

  \end{proof}



\end{enumerate}
\problem{Implementing IMP} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
See our IMP submission.
\problem{Debriefing} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
DO NOT FORGET THIS TIMEEEEE

\begin{enumerate}[(a)]
  \item
  \item
  \item
  \item
  \item
\end{enumerate}
\end{document}
